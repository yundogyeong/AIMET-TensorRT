{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobilenetv2 import mobilenet_v2\n",
    "# from resnet import resnet18\n",
    "# model = resnet18(pretrained=True)\n",
    "model = mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def evaluation(model, dataloader):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            start = time.time()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            results.append(end-start)\n",
    "\n",
    "    infer_time = np.mean(results)\n",
    "    print(infer_time)\n",
    "    print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013822449913507775\n",
      "Accuracy on test set: 93.91%\n"
     ]
    }
   ],
   "source": [
    "evaluation(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this step, AIMET inserts fake quantization ops in the model graph and configures them.\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "- Setting **default_output_bw** to 8 performs all activation quantizations in the model using integer 8-bit precision\n",
    "- Setting **default_param_bw** to 8 performs all parameter quantizations in the model using integer 8-bit precision\n",
    "\n",
    "See [QuantizationSimModel in the AIMET API documentation](https://quic.github.io/aimet-pages/AimetDocs/api_docs/torch_quantsim.html#aimet_torch.quantsim.QuantizationSimModel.compute_encodings) for a full explanation of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:09,724 - root - INFO - AIMET\n",
      "2024-12-26 04:07:14,332 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-12-26 04:07:14,352 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-12-26 04:07:14,353 - Quant - INFO - Unsupported op type Mean\n",
      "2024-12-26 04:07:14,356 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    }
   ],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.v1.quantsim import QuantizationSimModel\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 32, 32) \n",
    "\n",
    "dummy_input = dummy_input.cuda()\n",
    "\n",
    "sim = QuantizationSimModel(model=model,\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=8,\n",
    "                           default_param_bw=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Quantized Model Report\n",
      "-------------------------\n",
      "----------------------------------------------------------\n",
      "Layer: features.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.18.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.18.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.18.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: classifier.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: classifier.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_calibration_data(sim_model, data_loader):\n",
    "    batch_size = data_loader.batch_size\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    sim_model.eval()\n",
    "    samples = 1000\n",
    "\n",
    "    batch_cntr = 0\n",
    "    with torch.no_grad():\n",
    "        for input_data, target_data in data_loader:\n",
    "\n",
    "            inputs_batch = input_data.to(device)\n",
    "            sim_model(inputs_batch)\n",
    "\n",
    "            batch_cntr += 1\n",
    "            if (batch_cntr * batch_size) > samples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIMET has added quantizer nodes to the model graph, but before the sim model can be used for inference or training, scale and offset quantization parameters must be calculated for each quantizer node by passing unlabeled data samples through the model to collect range statistics. This process is sometimes referred to as calibration. AIMET refers to it as \"computing encodings\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021739624723603454\n",
      "Accuracy on test set: 90.00%\n"
     ]
    }
   ],
   "source": [
    "evaluation(sim.model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:52,681 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-12-26 04:07:52,700 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-12-26 04:07:52,701 - Quant - INFO - Unsupported op type Mean\n",
      "2024-12-26 04:07:52,704 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "2024-12-26 04:07:53,347 - Utils - INFO - Caching 1 batches from data loader at path location: /tmp/tmpg5elrk10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:53,396 - Quant - INFO - Started Optimizing weight rounding of module: features.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:53,941 - Quant - INFO - Started Optimizing weight rounding of module: features.1.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:54,121 - Quant - INFO - Started Optimizing weight rounding of module: features.1.conv.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:54,305 - Quant - INFO - Started Optimizing weight rounding of module: features.2.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:54,526 - Quant - INFO - Started Optimizing weight rounding of module: features.2.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:54,764 - Quant - INFO - Started Optimizing weight rounding of module: features.2.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:54,953 - Quant - INFO - Started Optimizing weight rounding of module: features.3.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:55,164 - Quant - INFO - Started Optimizing weight rounding of module: features.3.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:55,443 - Quant - INFO - Started Optimizing weight rounding of module: features.3.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:55,673 - Quant - INFO - Started Optimizing weight rounding of module: features.4.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:55,894 - Quant - INFO - Started Optimizing weight rounding of module: features.4.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:56,123 - Quant - INFO - Started Optimizing weight rounding of module: features.4.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:56,292 - Quant - INFO - Started Optimizing weight rounding of module: features.5.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:56,464 - Quant - INFO - Started Optimizing weight rounding of module: features.5.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:56,646 - Quant - INFO - Started Optimizing weight rounding of module: features.5.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:56,836 - Quant - INFO - Started Optimizing weight rounding of module: features.6.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:57,004 - Quant - INFO - Started Optimizing weight rounding of module: features.6.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:57,189 - Quant - INFO - Started Optimizing weight rounding of module: features.6.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:57,379 - Quant - INFO - Started Optimizing weight rounding of module: features.7.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:57,556 - Quant - INFO - Started Optimizing weight rounding of module: features.7.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:57,732 - Quant - INFO - Started Optimizing weight rounding of module: features.7.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:57,903 - Quant - INFO - Started Optimizing weight rounding of module: features.8.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:58,080 - Quant - INFO - Started Optimizing weight rounding of module: features.8.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:58,254 - Quant - INFO - Started Optimizing weight rounding of module: features.8.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:58,438 - Quant - INFO - Started Optimizing weight rounding of module: features.9.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:58,615 - Quant - INFO - Started Optimizing weight rounding of module: features.9.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:58,793 - Quant - INFO - Started Optimizing weight rounding of module: features.9.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:58,975 - Quant - INFO - Started Optimizing weight rounding of module: features.10.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:59,157 - Quant - INFO - Started Optimizing weight rounding of module: features.10.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:59,341 - Quant - INFO - Started Optimizing weight rounding of module: features.10.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:59,529 - Quant - INFO - Started Optimizing weight rounding of module: features.11.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:59,715 - Quant - INFO - Started Optimizing weight rounding of module: features.11.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:07:59,903 - Quant - INFO - Started Optimizing weight rounding of module: features.11.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:00,099 - Quant - INFO - Started Optimizing weight rounding of module: features.12.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:00,297 - Quant - INFO - Started Optimizing weight rounding of module: features.12.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:00,503 - Quant - INFO - Started Optimizing weight rounding of module: features.12.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:00,721 - Quant - INFO - Started Optimizing weight rounding of module: features.13.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:00,924 - Quant - INFO - Started Optimizing weight rounding of module: features.13.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:01,140 - Quant - INFO - Started Optimizing weight rounding of module: features.13.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:01,359 - Quant - INFO - Started Optimizing weight rounding of module: features.14.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:01,572 - Quant - INFO - Started Optimizing weight rounding of module: features.14.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:01,778 - Quant - INFO - Started Optimizing weight rounding of module: features.14.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:01,987 - Quant - INFO - Started Optimizing weight rounding of module: features.15.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:02,200 - Quant - INFO - Started Optimizing weight rounding of module: features.15.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:02,404 - Quant - INFO - Started Optimizing weight rounding of module: features.15.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:02,623 - Quant - INFO - Started Optimizing weight rounding of module: features.16.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:02,842 - Quant - INFO - Started Optimizing weight rounding of module: features.16.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:03,061 - Quant - INFO - Started Optimizing weight rounding of module: features.16.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:03,285 - Quant - INFO - Started Optimizing weight rounding of module: features.17.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:03,512 - Quant - INFO - Started Optimizing weight rounding of module: features.17.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:03,734 - Quant - INFO - Started Optimizing weight rounding of module: features.17.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:04,129 - Quant - INFO - Started Optimizing weight rounding of module: features.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:04,367 - Quant - INFO - Started Optimizing weight rounding of module: classifier.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:11<00:00, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:04,597 - Quant - INFO - Completed Adarounding Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.v1.adaround.adaround_weight import Adaround, AdaroundParameters\n",
    "import os\n",
    "\n",
    "data_loader = testloader\n",
    "params = AdaroundParameters(data_loader=data_loader, num_batches=1, default_num_iterations=64)\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 32, 32)\n",
    "dummy_input = dummy_input.cuda()\n",
    "\n",
    "os.makedirs('./output/', exist_ok=True)\n",
    "ada_model = Adaround.apply_adaround(model, dummy_input, params,\n",
    "                                    path=\"output\", \n",
    "                                    filename_prefix='adaround', \n",
    "                                    default_param_bw=4,\n",
    "                                    default_quant_scheme=QuantScheme.post_training_tf_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 04:08:05,926 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-12-26 04:08:05,944 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-12-26 04:08:05,945 - Quant - INFO - Unsupported op type Mean\n",
      "2024-12-26 04:08:05,949 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_921075/1984786770.py:7: DeprecationWarning: \u001b[31;21mQuantizationSimModel.set_and_freeze_param_encodings will be deprecated soon in the later versions. Use QuantizationSimModel.load_encodings instead.\u001b[0m\n",
      "  sim.set_and_freeze_param_encodings(encoding_path=os.path.join(\"output\", 'adaround.encodings'))\n"
     ]
    }
   ],
   "source": [
    "sim = QuantizationSimModel(model=ada_model,\n",
    "                           dummy_input=dummy_input,\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           default_output_bw=8, \n",
    "                           default_param_bw=4)\n",
    "\n",
    "sim.set_and_freeze_param_encodings(encoding_path=os.path.join(\"output\", 'adaround.encodings'))\n",
    "\n",
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02189430707617651\n",
      "Accuracy on test set: 91.85%\n"
     ]
    }
   ],
   "source": [
    "evaluation(sim.model, testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
