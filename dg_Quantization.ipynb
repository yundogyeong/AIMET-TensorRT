{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobilenetv2 import mobilenet_v2\n",
    "# from resnet import resnet18\n",
    "# model = resnet18(pretrained=True)\n",
    "model = mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def evaluation(model, dataloader):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            start = time.time()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            results.append(end-start)\n",
    "\n",
    "    infer_time = np.mean(results)\n",
    "    print(infer_time)\n",
    "    print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010886967936648598\n",
      "Accuracy on test set: 93.91%\n"
     ]
    }
   ],
   "source": [
    "evaluation(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:35,434 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-12-26 00:54:35,454 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-12-26 00:54:35,454 - Quant - INFO - Unsupported op type Mean\n",
      "2024-12-26 00:54:35,457 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    }
   ],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.v1.quantsim import QuantizationSimModel\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 32, 32) \n",
    "\n",
    "dummy_input = dummy_input.cuda()\n",
    "\n",
    "sim = QuantizationSimModel(model=model,\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=8,\n",
    "                           default_param_bw=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Quantized Model Report\n",
      "-------------------------\n",
      "----------------------------------------------------------\n",
      "Layer: features.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.0.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.1.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.18.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.18.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: Not quantized\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.18.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: classifier.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: classifier.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_calibration_data(sim_model, use_cuda):\n",
    "    data_loader = trainloader\n",
    "    batch_size = data_loader.batch_size\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    sim_model.eval()\n",
    "    samples = 1000\n",
    "\n",
    "    batch_cntr = 0\n",
    "    with torch.no_grad():\n",
    "        for input_data, target_data in data_loader:\n",
    "\n",
    "            inputs_batch = input_data.to(device)\n",
    "            sim_model(inputs_batch)\n",
    "\n",
    "            batch_cntr += 1\n",
    "            if (batch_cntr * batch_size) > samples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021007561985450455\n",
      "Accuracy on test set: 89.85%\n"
     ]
    }
   ],
   "source": [
    "evaluation(sim.model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:42,723 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-12-26 00:54:42,741 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-12-26 00:54:42,742 - Quant - INFO - Unsupported op type Mean\n",
      "2024-12-26 00:54:42,746 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "2024-12-26 00:54:43,364 - Utils - INFO - Caching 1 batches from data loader at path location: /tmp/tmpe6h05_ej\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:43,417 - Quant - INFO - Started Optimizing weight rounding of module: features.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:43,956 - Quant - INFO - Started Optimizing weight rounding of module: features.1.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:44,130 - Quant - INFO - Started Optimizing weight rounding of module: features.1.conv.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:44,306 - Quant - INFO - Started Optimizing weight rounding of module: features.2.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:44,523 - Quant - INFO - Started Optimizing weight rounding of module: features.2.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:44,733 - Quant - INFO - Started Optimizing weight rounding of module: features.2.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:44,919 - Quant - INFO - Started Optimizing weight rounding of module: features.3.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:45,126 - Quant - INFO - Started Optimizing weight rounding of module: features.3.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:45,399 - Quant - INFO - Started Optimizing weight rounding of module: features.3.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:45,621 - Quant - INFO - Started Optimizing weight rounding of module: features.4.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:45,837 - Quant - INFO - Started Optimizing weight rounding of module: features.4.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:46,056 - Quant - INFO - Started Optimizing weight rounding of module: features.4.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:46,224 - Quant - INFO - Started Optimizing weight rounding of module: features.5.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:46,391 - Quant - INFO - Started Optimizing weight rounding of module: features.5.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:46,569 - Quant - INFO - Started Optimizing weight rounding of module: features.5.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:46,759 - Quant - INFO - Started Optimizing weight rounding of module: features.6.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:46,930 - Quant - INFO - Started Optimizing weight rounding of module: features.6.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:47,109 - Quant - INFO - Started Optimizing weight rounding of module: features.6.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:47,286 - Quant - INFO - Started Optimizing weight rounding of module: features.7.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:47,459 - Quant - INFO - Started Optimizing weight rounding of module: features.7.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:47,632 - Quant - INFO - Started Optimizing weight rounding of module: features.7.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:47,799 - Quant - INFO - Started Optimizing weight rounding of module: features.8.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:47,971 - Quant - INFO - Started Optimizing weight rounding of module: features.8.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:48,143 - Quant - INFO - Started Optimizing weight rounding of module: features.8.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:48,322 - Quant - INFO - Started Optimizing weight rounding of module: features.9.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:48,494 - Quant - INFO - Started Optimizing weight rounding of module: features.9.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:48,672 - Quant - INFO - Started Optimizing weight rounding of module: features.9.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:48,851 - Quant - INFO - Started Optimizing weight rounding of module: features.10.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:49,032 - Quant - INFO - Started Optimizing weight rounding of module: features.10.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:49,213 - Quant - INFO - Started Optimizing weight rounding of module: features.10.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:49,397 - Quant - INFO - Started Optimizing weight rounding of module: features.11.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:49,580 - Quant - INFO - Started Optimizing weight rounding of module: features.11.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:49,766 - Quant - INFO - Started Optimizing weight rounding of module: features.11.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:49,960 - Quant - INFO - Started Optimizing weight rounding of module: features.12.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:50,157 - Quant - INFO - Started Optimizing weight rounding of module: features.12.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:50,359 - Quant - INFO - Started Optimizing weight rounding of module: features.12.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:50,572 - Quant - INFO - Started Optimizing weight rounding of module: features.13.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:50,769 - Quant - INFO - Started Optimizing weight rounding of module: features.13.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:50,977 - Quant - INFO - Started Optimizing weight rounding of module: features.13.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:51,191 - Quant - INFO - Started Optimizing weight rounding of module: features.14.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:51,394 - Quant - INFO - Started Optimizing weight rounding of module: features.14.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:51,595 - Quant - INFO - Started Optimizing weight rounding of module: features.14.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:51,800 - Quant - INFO - Started Optimizing weight rounding of module: features.15.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:52,005 - Quant - INFO - Started Optimizing weight rounding of module: features.15.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:52,207 - Quant - INFO - Started Optimizing weight rounding of module: features.15.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:52,419 - Quant - INFO - Started Optimizing weight rounding of module: features.16.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:52,626 - Quant - INFO - Started Optimizing weight rounding of module: features.16.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:52,834 - Quant - INFO - Started Optimizing weight rounding of module: features.16.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:53,048 - Quant - INFO - Started Optimizing weight rounding of module: features.17.conv.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:53,261 - Quant - INFO - Started Optimizing weight rounding of module: features.17.conv.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:53,474 - Quant - INFO - Started Optimizing weight rounding of module: features.17.conv.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:53,700 - Quant - INFO - Started Optimizing weight rounding of module: features.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:53,926 - Quant - INFO - Started Optimizing weight rounding of module: classifier.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:10<00:00, 13.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:54,144 - Quant - INFO - Completed Adarounding Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.v1.adaround.adaround_weight import Adaround, AdaroundParameters\n",
    "import os\n",
    "\n",
    "data_loader = testloader\n",
    "params = AdaroundParameters(data_loader=data_loader, num_batches=1, default_num_iterations=64)\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 32, 32)\n",
    "if use_cuda:\n",
    "    dummy_input = dummy_input.cuda()\n",
    "\n",
    "os.makedirs('./output/', exist_ok=True)\n",
    "ada_model = Adaround.apply_adaround(model, dummy_input, params,\n",
    "                                    path=\"output\", \n",
    "                                    filename_prefix='adaround', \n",
    "                                    default_param_bw=4,\n",
    "                                    default_quant_scheme=QuantScheme.post_training_tf_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-26 00:54:54,437 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-12-26 00:54:54,456 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-12-26 00:54:54,456 - Quant - INFO - Unsupported op type Mean\n",
      "2024-12-26 00:54:54,460 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_740042/1299832713.py:7: DeprecationWarning: \u001b[31;21mQuantizationSimModel.set_and_freeze_param_encodings will be deprecated soon in the later versions. Use QuantizationSimModel.load_encodings instead.\u001b[0m\n",
      "  sim.set_and_freeze_param_encodings(encoding_path=os.path.join(\"output\", 'adaround.encodings'))\n"
     ]
    }
   ],
   "source": [
    "sim = QuantizationSimModel(model=ada_model,\n",
    "                           dummy_input=dummy_input,\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           default_output_bw=8, \n",
    "                           default_param_bw=4)\n",
    "\n",
    "sim.set_and_freeze_param_encodings(encoding_path=os.path.join(\"output\", 'adaround.encodings'))\n",
    "\n",
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02107233940800534\n",
      "Accuracy on test set: 91.80%\n"
     ]
    }
   ],
   "source": [
    "evaluation(sim.model, testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
